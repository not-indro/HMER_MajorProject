{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":680,"status":"ok","timestamp":1707293775076,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"6V8RK57ky_2_"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from transformers import DistilBertTokenizer\n","from sklearn.model_selection import train_test_split\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1707293778253,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"kdIXCuvmzPfZ"},"outputs":[],"source":["# Step 1: Load and Preprocess Data\n","data_path = '/content/drive/MyDrive/Major_Project_Final-Yr/trainLatex.txt'\n","image_folder = '/content/drive/MyDrive/Major_Project_Final-Yr/off_image_train'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121296,"status":"ok","timestamp":1707293899530,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"H3B4hX7p6-Uz","outputId":"97534164-3bc8-4498-c7e7-7865f3c8051f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1705522138848,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"l1cXO7q_85pd","outputId":"efd4e633-0318-4dc0-af77-4884a03ff565"},"outputs":[{"name":"stdout","output_type":"stream","text":["drive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1707293899531,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"L1WWwk2n7_V9"},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1707293899531,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"DfwkOz5A50zA"},"outputs":[],"source":["\n","# Load label data from text file into a DataFrame\n","df = pd.read_csv(data_path, sep='\\t', names=['Image', 'Label'])\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707293900074,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"AglJ7JEW_gqn","outputId":"123c0088-8748-4a02-c00a-371a8c0309fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["                       Image  \\\n","0                   2_em_3_0   \n","1                   2_em_7_0   \n","2                  3_em_11_0   \n","3                  3_em_18_0   \n","4                  4_em_22_0   \n","...                      ...   \n","8830  TrainData2_26_sub_71_0   \n","8831  TrainData2_26_sub_73_0   \n","8832  TrainData2_26_sub_88_0   \n","8833  TrainData2_26_sub_95_0   \n","8834  TrainData2_26_sub_98_0   \n","\n","                                                  Label  \n","0                                               \\{ T \\}  \n","1                                               \\{ u \\}  \n","2                                       \\{ T _ { N } \\}  \n","3                                       \\{ I _ { k } \\}  \n","4                                      \\{ I , \\sigma \\}  \n","...                                                 ...  \n","8830      \\frac { \\sin B + \\sin C } { \\cos B + \\cos C }  \n","8831  \\alpha _ { n + 1 } - 3 \\beta = \\frac { 2 } { 3...  \n","8832                         3 0 \\times 2 9 x ^ { 2 8 }  \n","8833  \\sqrt { 1 + \\sqrt { 2 + \\sqrt { 3 + \\sqrt { 4 ...  \n","8834  \\lim _ { x \\rightarrow \\frac { 1 } { 4 } } \\fr...  \n","\n","[8835 rows x 2 columns]\n"]}],"source":["# Concatenate '_0.bmp' to the Image column\n","df['Image'] = df['Image'] + '_0'\n","\n","# Display the DataFrame\n","print(df)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":587},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1707293922670,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"eJzLWTP9pYu2","outputId":"f930b662-9e5b-483c-8bde-416d1013378a"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-d62dceff-e20e-4d91-8ebf-e40280e1245a\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eImage\u003c/th\u003e\n","      \u003cth\u003eLabel\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e2_em_3_0\u003c/td\u003e\n","      \u003ctd\u003e\\{ T \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e2_em_7_0\u003c/td\u003e\n","      \u003ctd\u003e\\{ u \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e3_em_11_0\u003c/td\u003e\n","      \u003ctd\u003e\\{ T _ { N } \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e3_em_18_0\u003c/td\u003e\n","      \u003ctd\u003e\\{ I _ { k } \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e4_em_22_0\u003c/td\u003e\n","      \u003ctd\u003e\\{ I , \\sigma \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e5\u003c/th\u003e\n","      \u003ctd\u003e4_em_26_0\u003c/td\u003e\n","      \u003ctd\u003eG \\neq \\{ e \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e6\u003c/th\u003e\n","      \u003ctd\u003e4_em_27_0\u003c/td\u003e\n","      \u003ctd\u003e\\{ \\sigma , F \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7\u003c/th\u003e\n","      \u003ctd\u003e5_em_30_0\u003c/td\u003e\n","      \u003ctd\u003e\\{ c _ { s } \\}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e8\u003c/th\u003e\n","      \u003ctd\u003e7_em_59_0\u003c/td\u003e\n","      \u003ctd\u003e\\forall g \\in G\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e9\u003c/th\u003e\n","      \u003ctd\u003e8_em_62_0\u003c/td\u003e\n","      \u003ctd\u003e\\sigma \\in G\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e10\u003c/th\u003e\n","      \u003ctd\u003e8_em_65_0\u003c/td\u003e\n","      \u003ctd\u003e\\forall i \\in I\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11\u003c/th\u003e\n","      \u003ctd\u003e9_em_71_0\u003c/td\u003e\n","      \u003ctd\u003eT ^ { \\mu } _ { \\mu }\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e12\u003c/th\u003e\n","      \u003ctd\u003e9_em_76_0\u003c/td\u003e\n","      \u003ctd\u003e\\phi \\in S\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e13\u003c/th\u003e\n","      \u003ctd\u003e9_em_77_0\u003c/td\u003e\n","      \u003ctd\u003eH \\in P\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e14\u003c/th\u003e\n","      \u003ctd\u003e10_em_81_0\u003c/td\u003e\n","      \u003ctd\u003e\\Delta H _ { I }\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e15\u003c/th\u003e\n","      \u003ctd\u003e10_em_86_0\u003c/td\u003e\n","      \u003ctd\u003eM \\in S\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e16\u003c/th\u003e\n","      \u003ctd\u003e10_em_87_0\u003c/td\u003e\n","      \u003ctd\u003e\\sigma \\in X\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e17\u003c/th\u003e\n","      \u003ctd\u003e10_em_89_0\u003c/td\u003e\n","      \u003ctd\u003eT \\in E\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e18\u003c/th\u003e\n","      \u003ctd\u003e11_em_91_0\u003c/td\u003e\n","      \u003ctd\u003e\\Delta L\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e19\u003c/th\u003e\n","      \u003ctd\u003e11_em_95_0\u003c/td\u003e\n","      \u003ctd\u003e\\forall u \\in V\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d62dceff-e20e-4d91-8ebf-e40280e1245a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-d62dceff-e20e-4d91-8ebf-e40280e1245a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d62dceff-e20e-4d91-8ebf-e40280e1245a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-23580253-007e-4ba4-82fd-de348b30c445\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-23580253-007e-4ba4-82fd-de348b30c445')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-23580253-007e-4ba4-82fd-de348b30c445 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["         Image                  Label\n","0     2_em_3_0                \\{ T \\}\n","1     2_em_7_0                \\{ u \\}\n","2    3_em_11_0        \\{ T _ { N } \\}\n","3    3_em_18_0        \\{ I _ { k } \\}\n","4    4_em_22_0       \\{ I , \\sigma \\}\n","5    4_em_26_0         G \\neq \\{ e \\}\n","6    4_em_27_0       \\{ \\sigma , F \\}\n","7    5_em_30_0        \\{ c _ { s } \\}\n","8    7_em_59_0        \\forall g \\in G\n","9    8_em_62_0           \\sigma \\in G\n","10   8_em_65_0        \\forall i \\in I\n","11   9_em_71_0  T ^ { \\mu } _ { \\mu }\n","12   9_em_76_0             \\phi \\in S\n","13   9_em_77_0                H \\in P\n","14  10_em_81_0       \\Delta H _ { I }\n","15  10_em_86_0                M \\in S\n","16  10_em_87_0           \\sigma \\in X\n","17  10_em_89_0                T \\in E\n","18  11_em_91_0               \\Delta L\n","19  11_em_95_0        \\forall u \\in V"]},"execution_count":9,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["No charts were generated by quickchart\n"]}],"source":["df.head(20)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":663,"status":"ok","timestamp":1707294231884,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"4Z7aIqdWK2qt","outputId":"0a79aa26-1829-4e68-c0ef-d90da5b0f92d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image Title: 2_em_3_0.bmp and Latex Code is: \\{ T \\}\n"]}],"source":["print(f\"Image Title: {df['Image'][0]}.bmp and Latex Code is: {df['Label'][0]}\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":590,"status":"ok","timestamp":1707294253910,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"M8-BzuqEMIcg","outputId":"438da5b2-9242-432b-89f4-6077e02b6f4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image Title: 4_em_22_0 and Latex Code is: \\{ I , \\sigma \\}\n"]}],"source":["image_title = '4_em_22_0'\n","\n","# Use loc to locate the row with the specified image title and retrieve the label\n","label = df.loc[df['Image'] == image_title, 'Label'].values[0]\n","\n","print(f\"Image Title: {image_title} and Latex Code is: {label}\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":583,"status":"ok","timestamp":1707294392287,"user":{"displayName":"2020CSB038 ASTIK_GORAI","userId":"17172243839223874929"},"user_tz":-330},"id":"qplDB4eONGA3","outputId":"d62f7cd3-345c-4592-9a4e-d079bb8172b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image Title: 4_em_22_0 and Latex Code is: \\{ I , \\sigma \\}\n"]}],"source":["# Assuming df is your DataFrame with 'Image' and 'Label' columns\n","image_label_dict = dict(zip(df['Image'], df['Label']))\n","\n","# Replace 'your_image_title.jpg' with the actual image title you're looking for\n","image_title = '4_em_22_0'\n","\n","# Access the label using the image title\n","if image_title in image_label_dict:\n","    label = image_label_dict[image_title]\n","    print(f\"Image Title: {image_title} and Latex Code is: {label}\")\n","else:\n","    print(f\"Image Title '{image_title}' not found in the DataFrame.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7yDzBQMP6XBG"},"outputs":[],"source":["# Initialize lists to store preprocessed data\n","images = []\n","labels = []\n","\n","# Load and preprocess images (customize based on your requirements)\n","for image_filename in df['Image']:\n","    image_path = os.path.join(image_folder, f'{image_filename}.bmp')\n","    image = tf.image.decode_bmp(tf.io.read_file(image_path))\n","    image = tf.image.resize(image, (224, 224))  # Adjust size as needed\n","    image = tf.image.convert_image_dtype(image, tf.float32)  # Normalize\n","    images.append(image)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCduIcyW7uk9"},"outputs":[],"source":["# Tokenize and encode labels\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","labels = tokenizer(df['Label'].tolist(), return_tensors='tf', padding=True, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAA2nH1eF9Ns"},"outputs":[],"source":["# # Step 2: Create TensorFlow Dataset ## Error\n","# X_train, X_valid, y_train, y_valid = train_test_split(images, labels['input_ids'], test_size=0.2)\n","# # Step 2: Create TensorFlow Dataset\n","# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train['input_ids']))\n","# valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid['input_ids']))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUpu1_7sQh0D"},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Assuming 'images' is your input data and 'labels' is a dictionary with 'input_ids' as a key\n","# Split the data into training and validation sets\n","X_train, X_valid, y_train, y_valid = train_test_split(images, labels['input_ids'], test_size=0.2)\n","\n","# Step 2: Create TensorFlow Dataset\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n","\n","# Optionally, you can shuffle and batch your datasets\n","# Example:\n","batch_size = 32\n","train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)\n","valid_dataset = valid_dataset.batch(batch_size)\n","\n","# Convert TensorFlow dataset to Pandas DataFrame\n","def dataset_to_dataframe(dataset):\n","    data = {'images': [], 'labels': []}\n","    for images, labels in dataset:\n","        # Assuming images and labels are NumPy arrays\n","        data['images'].extend(images.numpy())\n","        data['labels'].extend(labels.numpy())\n","\n","    return pd.DataFrame(data)\n","\n","# Convert train_dataset and valid_dataset to DataFrames\n","train_dataframe = dataset_to_dataframe(train_dataset)\n","valid_dataframe = dataset_to_dataframe(valid_dataset)\n","\n","# Display the first few rows of the DataFrames\n","print(\"Train DataFrame:\")\n","print(train_dataframe.head())\n","\n","print(\"\\nValid DataFrame:\")\n","print(valid_dataframe.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfitnDQ0HUPb"},"outputs":[],"source":["# Create TensorFlow Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((images, labels['input_ids']))\n","\n","# Split the dataset\n","train_size = int(0.8 * len(images))\n","train_dataset = dataset.take(train_size).batch(32)\n","valid_dataset = dataset.skip(train_size).batch(32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqY5nSMKQjSc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzdFQnByQahw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cH1CFgIxGESU"},"outputs":[],"source":["## How To Acess the Images and their Corresponding Labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRYFDqHhHtYg"},"outputs":[],"source":["print(images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4cd5ZmPI7al"},"outputs":[],"source":["from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFjc4WA-IeL7"},"outputs":[],"source":["# Print values from the dataset\n","for image, label in dataset.take(2):  # Print values for the first 2 samples\n","    print(\"Image Tensor Shape:\", image.shape)\n","    print(\"Label Tensor Shape:\", label.shape)\n","    plt.imshow(image)\n","    print(label)\n","    # print(\"Label Tensor Values:\", label.numpy())\n","    print(\"------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdYrWFlSKaZV"},"outputs":[],"source":["from transformers import TFAutoModel, AutoTokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu2yiMs0I4Q0"},"outputs":[],"source":["# Build a simple transformer-based model\n","transformer_model = TFAutoModel.from_pretrained('bert-base-uncased')\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(224, 224, 3)),  # Image input\n","    tf.keras.layers.Flatten(),\n","    transformer_model,\n","    tf.keras.layers.Dense(units=512, activation='relu'),\n","    tf.keras.layers.Dense(units=len(tokenizer.get_vocab()), activation='softmax')  # Adjust output units based on your task\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49MWE7KFKyVv"},"outputs":[],"source":["\n","# Build a simple transformer-based model\n","transformer_model = TFAutoModel.from_pretrained('bert-base-uncased')\n","\n","# Define model inputs\n","image_input = tf.keras.layers.Input(shape=(224, 224, 3), name=\"image_input\")\n","label_input = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"label_input\")\n","\n","# Process image through the transformer model\n","image_embedding = transformer_model(image_input)[\"last_hidden_state\"][:, 0, :]\n","\n","# Concatenate image embedding with label input\n","combined_embedding = tf.keras.layers.Concatenate(axis=1)([image_embedding, transformer_model(label_input)[\"last_hidden_state\"][:, 0, :]])\n","\n","# Fully connected layers for prediction\n","dense_layer = tf.keras.layers.Dense(units=512, activation='relu')(combined_embedding)\n","output_layer = tf.keras.layers.Dense(units=len(tokenizer.get_vocab()), activation='softmax')(dense_layer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIOgllMEKNi5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NXI7mIMfTuno"},"source":["## **Transformer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mc3zYzO4T3BX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdTblxTF_byq"},"outputs":[],"source":["import torch\n","import math\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"z4pEbKX45rMu"},"source":["### Encoder Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGyFD9n_5tjQ"},"outputs":[],"source":["def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n","    print(f\"scaled.size() : {scaled.size()}\")\n","    if mask is not None:\n","        print(f\"-- ADDING MASK of shape {mask.size()} --\")\n","        # Broadcasting add. So just the last N dimensions need to match\n","        scaled += mask\n","    attention = F.softmax(scaled, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention\n","\n","class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.head_dim = d_model // num_heads\n","        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n","        self.linear_layer = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, max_sequence_length, d_model = x.size()\n","        print(f\"x.size(): {x.size()}\")\n","        qkv = self.qkv_layer(x)\n","        print(f\"qkv.size(): {qkv.size()}\")\n","        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n","        print(f\"qkv.size(): {qkv.size()}\")\n","        qkv = qkv.permute(0, 2, 1, 3)\n","        print(f\"qkv.size(): {qkv.size()}\")\n","        q, k, v = qkv.chunk(3, dim=-1)\n","        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n","        values, attention = scaled_dot_product(q, k, v, mask)\n","        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n","        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n","        print(f\"values.size(): {values.size()}\")\n","        out = self.linear_layer(values)\n","        print(f\"out.size(): {out.size()}\")\n","        return out\n","\n","\n","class LayerNormalization(nn.Module):\n","    def __init__(self, parameters_shape, eps=1e-5):\n","        super().__init__()\n","        self.parameters_shape=parameters_shape\n","        self.eps=eps\n","        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n","        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n","\n","    def forward(self, inputs):\n","        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n","        mean = inputs.mean(dim=dims, keepdim=True)\n","        print(f\"Mean ({mean.size()})\")\n","        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n","        std = (var + self.eps).sqrt()\n","        print(f\"Standard Deviation  ({std.size()})\")\n","        y = (inputs - mean) / std\n","        print(f\"y: {y.size()}\")\n","        out = self.gamma * y  + self.beta\n","        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n","        print(f\"out: {out.size()}\")\n","        return out\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","\n","    def __init__(self, d_model, hidden, drop_prob=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.linear1 = nn.Linear(d_model, hidden)\n","        self.linear2 = nn.Linear(hidden, d_model)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        print(f\"x after first linear layer: {x.size()}\")\n","        x = self.relu(x)\n","        print(f\"x after activation: {x.size()}\")\n","        x = self.dropout(x)\n","        print(f\"x after dropout: {x.size()}\")\n","        x = self.linear2(x)\n","        print(f\"x after 2nd linear layer: {x.size()}\")\n","        return x\n","\n","\n","class EncoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n","        super(EncoderLayer, self).__init__()\n","        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n","        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n","        self.dropout1 = nn.Dropout(p=drop_prob)\n","        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n","        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n","        self.dropout2 = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x):\n","        residual_x = x\n","        print(\"------- ATTENTION 1 ------\")\n","        x = self.attention(x, mask=None)\n","        print(\"------- DROPOUT 1 ------\")\n","        x = self.dropout1(x)\n","        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n","        x = self.norm1(x + residual_x)\n","        residual_x = x\n","        print(\"------- ATTENTION 2 ------\")\n","        x = self.ffn(x)\n","        print(\"------- DROPOUT 2 ------\")\n","        x = self.dropout2(x)\n","        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n","        x = self.norm2(x + residual_x)\n","        return x\n","\n","class Encoder(nn.Module):\n","    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n","        super().__init__()\n","        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n","                                     for _ in range(num_layers)])\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"r4-BDqDL6Bqw"},"source":["### Decoder Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcvU40N06EOy"},"outputs":[],"source":["\n","class MultiHeadCrossAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.head_dim = d_model // num_heads\n","        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n","        self.q_layer = nn.Linear(d_model , d_model)\n","        self.linear_layer = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x, y, mask):\n","        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n","        kv = self.kv_layer(x)\n","        q = self.q_layer(y)\n","        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n","        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n","        kv = kv.permute(0, 2, 1, 3)\n","        q = q.permute(0, 2, 1, 3)\n","        k, v = kv.chunk(2, dim=-1)\n","        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n","        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n","        out = self.linear_layer(values)\n","        return out\n","class DecoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n","        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n","        self.dropout1 = nn.Dropout(p=drop_prob)\n","        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n","        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n","        self.dropout2 = nn.Dropout(p=drop_prob)\n","        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n","        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n","        self.dropout3 = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x, y, decoder_mask):\n","        _y = y # 30 x 200 x 512\n","        print(\"MASKED SELF ATTENTION\")\n","        y = self.self_attention(y, mask=decoder_mask) # 30 x 200 x 512\n","        print(\"DROP OUT 1\")\n","        y = self.dropout1(y) # 30 x 200 x 512\n","        print(\"ADD + LAYER NORMALIZATION 1\")\n","        y = self.norm1(y + _y) # 30 x 200 x 512\n","\n","        _y = y # 30 x 200 x 512\n","        print(\"CROSS ATTENTION\")\n","        y = self.encoder_decoder_attention(x, y, mask=None) #30 x 200 x 512\n","        print(\"DROP OUT 2\")  #30 x 200 x 512\n","        y = self.dropout2(y)\n","        print(\"ADD + LAYER NORMALIZATION 2\")\n","        y = self.norm2(y + _y)  #30 x 200 x 512\n","\n","        _y = y  #30 x 200 x 512\n","        print(\"FEED FORWARD 1\")\n","        y = self.ffn(y) #30 x 200 x 512\n","        print(\"DROP OUT 3\")\n","        y = self.dropout3(y) #30 x 200 x 512\n","        print(\"ADD + LAYER NORMALIZATION 3\")\n","        y = self.norm3(y + _y) #30 x 200 x 512\n","        return y #30 x 200 x 512\n","class SequentialDecoder(nn.Sequential):\n","    def forward(self, *inputs):\n","        x, y, mask = inputs\n","        for module in self._modules.values():\n","            y = module(x, y, mask) #30 x 200 x 512\n","        return y\n","\n","class Decoder(nn.Module):\n","    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n","        super().__init__()\n","        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n","                                          for _ in range(num_layers)])\n","\n","    def forward(self, x, y, mask):\n","        #x : 30 x 200 x 512\n","        #y : 30 x 200 x 512\n","        #mask : 200 x 200\n","        y = self.layers(x, y, mask)\n","        return y #30 x 200 x 512"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSpMZjMA9TCt"},"outputs":[],"source":["class transformer_model(nn.Module):\n","    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_encoder_layers, num_decoder_layers):\n","        super(transformer_model, self).__init__()\n","        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers=num_encoder_layers)\n","        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers=num_decoder_layers)\n","\n","    def forward(self, x, y, mask):\n","        # x: image data, y: label data, mask: mask for decoder self-attention\n","        encoder_output = self.encoder(x)\n","        decoder_output = self.decoder(encoder_output, y, mask)\n","        return decoder_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIt5BmZIAkbq"},"outputs":[],"source":["d_model = 512  # Adjust according to your model configuration\n","ffn_hidden = 2048  # Adjust according to your model configuration\n","num_heads = 8  # Adjust according to your model configuration\n","drop_prob = 0.1  # Adjust according to your model configuration\n","num_encoder_layers = 6  # Adjust according to your model configuration\n","num_decoder_layers = 6  # Adjust according to your model configuration\n","\n","transformer_model_instance = transformer_model(\n","    d_model, ffn_hidden, num_heads, drop_prob, num_encoder_layers, num_decoder_layers\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NC837aX8_VC8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","# Assuming you have a custom loss function, let's call it 'custom_loss'\n","# Assuming your images and labels are tensors\n","# Adjust batch size according to your memory capacity\n","batch_size = 32\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, images, labels):\n","        self.images = images\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        image = torch.tensor(self.images[index], dtype=torch.float32)\n","        label = torch.tensor(self.labels[index], dtype=torch.long)\n","        return image, label\n","\n","train_dataset = CustomDataset(images[:train_size], labels['input_ids'][:train_size])\n","valid_dataset = CustomDataset(images[train_size:], labels['input_ids'][train_size:])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Now, you can use these loaders to train your PyTorch model\n","# Assuming you have initialized your model as 'transformer_model'\n","optimizer = optim.Adam(transformer_model_instance.parameters(), lr=0.001)\n","num_epochs = 10\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    transformer_model.train()\n","    total_loss = 0.0\n","\n","    for batch_images, batch_labels in train_loader:\n","        # Forward pass\n","        outputs = transformer_model(batch_images, batch_labels, mask=None)\n","\n","        # Calculate the loss\n","        loss = custom_loss(outputs, batch_labels)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    average_loss = total_loss / len(train_loader)\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n","\n","    # Validation loop\n","    transformer_model.eval()\n","    with torch.no_grad():\n","        total_val_loss = 0.0\n","\n","        for batch_images, batch_labels in valid_loader:\n","            # Forward pass\n","            outputs = transformer_model(batch_images, batch_labels, mask=None)\n","\n","            # Calculate the loss\n","            val_loss = custom_loss(outputs, batch_labels)\n","\n","            total_val_loss += val_loss.item()\n","\n","        average_val_loss = total_val_loss / len(valid_loader)\n","        print(f'Validation Loss: {average_val_loss:.4f}')\n","\n","# After training, you can save your model if needed\n","torch.save(transformer_model.state_dict(), 'transformer_model.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybVsTSbV7kNr"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","# Assuming your Transformer model is already defined, let's call it 'transformer_model'\n","# Also, assuming you have a custom loss function, let's call it 'custom_loss'\n","# Define your optimizer, learning rate, and other training configurations\n","optimizer = optim.Adam(TransformerModel.parameters(), lr=0.001)\n","num_epochs = 10\n","\n","# Assuming your DataLoader is named 'train_loader' and 'valid_loader'\n","# Adjust batch size according to your memory capacity\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    TransformerModel.train()\n","    total_loss = 0.0\n","\n","    for batch_images, batch_labels in train_loader:\n","        # Assuming batch_images is your image data and batch_labels is your corresponding label data\n","        # Perform any necessary pre-processing on your input data\n","\n","        # Forward pass\n","        outputs = TransformerModel(batch_images, batch_labels)\n","\n","        # Calculate the loss\n","        loss = custom_loss(outputs, batch_labels)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    average_loss = total_loss / len(train_loader)\n","\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n","\n","    # Validation loop\n","    TransformerModel.eval()\n","    with torch.no_grad():\n","        total_val_loss = 0.0\n","\n","        for batch_images, batch_labels in valid_loader:\n","            # Assuming batch_images is your image data and batch_labels is your corresponding label data\n","            # Perform any necessary pre-processing on your input data\n","\n","            # Forward pass\n","            outputs = TransformerModel(batch_images, batch_labels)\n","\n","            # Calculate the loss\n","            val_loss = custom_loss(outputs, batch_labels)\n","\n","            total_val_loss += val_loss.item()\n","\n","        average_val_loss = total_val_loss / len(valid_loader)\n","        print(f'Validation Loss: {average_val_loss:.4f}')\n","\n","# After training, you can save your model if needed\n","torch.save(TransformerModel.state_dict(), 'TransformerModel.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eru2_y568xUV"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}